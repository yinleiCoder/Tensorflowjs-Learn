{"id":"../node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js","dependencies":[{"name":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js.map","includedInParent":true,"mtime":1595498163399},{"name":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\src\\optimizers\\optimizer_constructors.ts","includedInParent":true,"mtime":1595498163399},{"name":"E:\\VisualStudioCodeProjects\\js机器学习\\package.json","includedInParent":true,"mtime":1595925880834},{"name":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\package.json","includedInParent":true,"mtime":1595498163399},{"name":"./adadelta_optimizer","loc":{"line":17,"column":34},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\adadelta_optimizer.js"},{"name":"./adagrad_optimizer","loc":{"line":18,"column":33},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\adagrad_optimizer.js"},{"name":"./adam_optimizer","loc":{"line":19,"column":30},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\adam_optimizer.js"},{"name":"./adamax_optimizer","loc":{"line":20,"column":32},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\adamax_optimizer.js"},{"name":"./momentum_optimizer","loc":{"line":21,"column":34},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\momentum_optimizer.js"},{"name":"./rmsprop_optimizer","loc":{"line":22,"column":33},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\rmsprop_optimizer.js"},{"name":"./sgd_optimizer","loc":{"line":23,"column":29},"parent":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\optimizer_constructors.js","resolved":"E:\\VisualStudioCodeProjects\\js机器学习\\node_modules\\@tensorflow\\tfjs-core\\dist\\optimizers\\sgd_optimizer.js"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.OptimizerConstructors = void 0;\n\nvar _adadelta_optimizer = require(\"./adadelta_optimizer\");\n\nvar _adagrad_optimizer = require(\"./adagrad_optimizer\");\n\nvar _adam_optimizer = require(\"./adam_optimizer\");\n\nvar _adamax_optimizer = require(\"./adamax_optimizer\");\n\nvar _momentum_optimizer = require(\"./momentum_optimizer\");\n\nvar _rmsprop_optimizer = require(\"./rmsprop_optimizer\");\n\nvar _sgd_optimizer = require(\"./sgd_optimizer\");\n\n/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nclass OptimizerConstructors {\n  /**\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static sgd(learningRate) {\n    return new _sgd_optimizer.SGDOptimizer(learningRate);\n  }\n  /**\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static momentum(learningRate, momentum, useNesterov = false) {\n    return new _momentum_optimizer.MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n  /**\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {\n    return new _rmsprop_optimizer.RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n  }\n  /**\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {\n    return new _adam_optimizer.AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n  /**\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adadelta(learningRate = .001, rho = .95, epsilon = null) {\n    return new _adadelta_optimizer.AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n  /**\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {\n    return new _adamax_optimizer.AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n  /**\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   */\n\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n\n\n  static adagrad(learningRate, initialAccumulatorValue = 0.1) {\n    return new _adagrad_optimizer.AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n\n}\n\nexports.OptimizerConstructors = OptimizerConstructors;"},"sourceMaps":{"js":{"mappings":[{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":18,"column":0},"generated":{"line":8,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":19,"column":0},"generated":{"line":10,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":20,"column":0},"generated":{"line":12,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":21,"column":0},"generated":{"line":14,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":22,"column":0},"generated":{"line":16,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":23,"column":0},"generated":{"line":18,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":24,"column":0},"generated":{"line":20,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":1,"column":0},"generated":{"line":22,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":6},"generated":{"line":38,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":13},"generated":{"line":38,"column":6}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":6},"generated":{"line":38,"column":27}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":34},"generated":{"line":38,"column":28}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":27,"column":2},"generated":{"line":39,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":62,"column":2},"generated":{"line":75,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":2},"generated":{"line":78,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":9},"generated":{"line":78,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":2},"generated":{"line":78,"column":12}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":13},"generated":{"line":78,"column":13}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":2},"generated":{"line":78,"column":25}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":65,"column":33},"generated":{"line":78,"column":27}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":4},"generated":{"line":79,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":11},"generated":{"line":79,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":15},"generated":{"line":79,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":11},"generated":{"line":79,"column":42}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":28},"generated":{"line":79,"column":43}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":11},"generated":{"line":79,"column":55}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":66,"column":4},"generated":{"line":79,"column":56}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":67,"column":3},"generated":{"line":80,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":69,"column":2},"generated":{"line":81,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":82,"column":2},"generated":{"line":95,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":2},"generated":{"line":100,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":9},"generated":{"line":100,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":2},"generated":{"line":100,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":18},"generated":{"line":100,"column":18}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":2},"generated":{"line":100,"column":30}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":40},"generated":{"line":100,"column":32}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":2},"generated":{"line":100,"column":40}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":58},"generated":{"line":100,"column":42}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":69},"generated":{"line":100,"column":53}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":72},"generated":{"line":100,"column":56}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":2},"generated":{"line":100,"column":61}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":85,"column":77},"generated":{"line":100,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":4},"generated":{"line":101,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":11},"generated":{"line":101,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":15},"generated":{"line":101,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":11},"generated":{"line":101,"column":52}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":33},"generated":{"line":101,"column":53}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":11},"generated":{"line":101,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":47},"generated":{"line":101,"column":67}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":11},"generated":{"line":101,"column":75}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":57},"generated":{"line":101,"column":77}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":11},"generated":{"line":101,"column":88}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":87,"column":4},"generated":{"line":101,"column":89}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":88,"column":3},"generated":{"line":102,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":90,"column":2},"generated":{"line":103,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":108,"column":2},"generated":{"line":122,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":9},"generated":{"line":127,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":16}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":6},"generated":{"line":127,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":29}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":28},"generated":{"line":127,"column":31}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":33},"generated":{"line":127,"column":36}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":36},"generated":{"line":127,"column":39}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":41}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":40},"generated":{"line":127,"column":43}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":48},"generated":{"line":127,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":51},"generated":{"line":127,"column":54}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":56},"generated":{"line":127,"column":59}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":56},"generated":{"line":127,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":112,"column":74},"generated":{"line":127,"column":69}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":73}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":113,"column":6},"generated":{"line":127,"column":75}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":113,"column":14},"generated":{"line":127,"column":83}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":113,"column":17},"generated":{"line":127,"column":86}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":111,"column":2},"generated":{"line":127,"column":91}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":113,"column":22},"generated":{"line":127,"column":93}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":4},"generated":{"line":128,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":15},"generated":{"line":128,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":50}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":115,"column":8},"generated":{"line":128,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":115,"column":22},"generated":{"line":128,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":70}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":115,"column":29},"generated":{"line":128,"column":72}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":115,"column":39},"generated":{"line":128,"column":82}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":89}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":115,"column":48},"generated":{"line":128,"column":91}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":11},"generated":{"line":128,"column":99}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":114,"column":4},"generated":{"line":128,"column":100}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":116,"column":3},"generated":{"line":129,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":118,"column":2},"generated":{"line":130,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":128,"column":2},"generated":{"line":141,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":2},"generated":{"line":146,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":9},"generated":{"line":146,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":2},"generated":{"line":146,"column":13}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":6},"generated":{"line":146,"column":14}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":18},"generated":{"line":146,"column":26}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":21},"generated":{"line":146,"column":29}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":2},"generated":{"line":146,"column":34}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":28},"generated":{"line":146,"column":36}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":33},"generated":{"line":146,"column":41}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":36},"generated":{"line":146,"column":44}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":2},"generated":{"line":146,"column":47}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":41},"generated":{"line":146,"column":49}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":46},"generated":{"line":146,"column":54}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":132,"column":49},"generated":{"line":146,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":2},"generated":{"line":146,"column":62}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":133,"column":6},"generated":{"line":146,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":133,"column":6},"generated":{"line":146,"column":71}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":133,"column":24},"generated":{"line":146,"column":74}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":131,"column":2},"generated":{"line":146,"column":78}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":133,"column":28},"generated":{"line":146,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":4},"generated":{"line":147,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":11},"generated":{"line":147,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":15},"generated":{"line":147,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":11},"generated":{"line":147,"column":44}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":29},"generated":{"line":147,"column":45}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":11},"generated":{"line":147,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":43},"generated":{"line":147,"column":59}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":11},"generated":{"line":147,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":50},"generated":{"line":147,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":11},"generated":{"line":147,"column":71}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":57},"generated":{"line":147,"column":73}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":11},"generated":{"line":147,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":134,"column":4},"generated":{"line":147,"column":81}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":135,"column":3},"generated":{"line":148,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":137,"column":2},"generated":{"line":149,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":147,"column":2},"generated":{"line":160,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":2},"generated":{"line":165,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":9},"generated":{"line":165,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":2},"generated":{"line":165,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":18},"generated":{"line":165,"column":18}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":30},"generated":{"line":165,"column":30}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":33},"generated":{"line":165,"column":33}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":2},"generated":{"line":165,"column":37}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":39},"generated":{"line":165,"column":39}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":42},"generated":{"line":165,"column":42}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":45},"generated":{"line":165,"column":45}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":2},"generated":{"line":165,"column":48}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":50},"generated":{"line":165,"column":50}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":50},"generated":{"line":165,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":68},"generated":{"line":165,"column":60}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":2},"generated":{"line":165,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":150,"column":72},"generated":{"line":165,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":4},"generated":{"line":166,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":11},"generated":{"line":166,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":15},"generated":{"line":166,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":11},"generated":{"line":166,"column":52}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":33},"generated":{"line":166,"column":53}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":11},"generated":{"line":166,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":47},"generated":{"line":166,"column":67}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":11},"generated":{"line":166,"column":70}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":52},"generated":{"line":166,"column":72}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":11},"generated":{"line":166,"column":79}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":152,"column":4},"generated":{"line":166,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":153,"column":3},"generated":{"line":167,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":155,"column":2},"generated":{"line":168,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":166,"column":2},"generated":{"line":180,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":9},"generated":{"line":185,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":6},"generated":{"line":185,"column":16}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":18},"generated":{"line":185,"column":28}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":21},"generated":{"line":185,"column":31}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":36}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":28},"generated":{"line":185,"column":38}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":33},"generated":{"line":185,"column":43}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":36},"generated":{"line":185,"column":46}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":49}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":41},"generated":{"line":185,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":46},"generated":{"line":185,"column":56}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":49},"generated":{"line":185,"column":59}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":64}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":56},"generated":{"line":185,"column":66}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":56},"generated":{"line":185,"column":73}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":170,"column":74},"generated":{"line":185,"column":76}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":80}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":171,"column":6},"generated":{"line":185,"column":82}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":171,"column":11},"generated":{"line":185,"column":87}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":171,"column":14},"generated":{"line":185,"column":90}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":169,"column":2},"generated":{"line":185,"column":93}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":171,"column":17},"generated":{"line":185,"column":95}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":4},"generated":{"line":186,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":15},"generated":{"line":186,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":48}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":31},"generated":{"line":186,"column":49}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":61}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":45},"generated":{"line":186,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":68}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":52},"generated":{"line":186,"column":70}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":75}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":59},"generated":{"line":186,"column":77}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":84}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":68},"generated":{"line":186,"column":86}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":11},"generated":{"line":186,"column":91}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":172,"column":4},"generated":{"line":186,"column":92}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":173,"column":3},"generated":{"line":187,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":175,"column":2},"generated":{"line":188,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":189,"column":2},"generated":{"line":203,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":2},"generated":{"line":208,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":9},"generated":{"line":208,"column":9}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":2},"generated":{"line":208,"column":16}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":17},"generated":{"line":208,"column":17}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":2},"generated":{"line":208,"column":29}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":39},"generated":{"line":208,"column":31}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":62},"generated":{"line":208,"column":54}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":65},"generated":{"line":208,"column":57}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":2},"generated":{"line":208,"column":60}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":192,"column":68},"generated":{"line":208,"column":62}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":4},"generated":{"line":209,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":11},"generated":{"line":209,"column":11}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":15},"generated":{"line":209,"column":15}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":11},"generated":{"line":209,"column":50}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":32},"generated":{"line":209,"column":51}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":11},"generated":{"line":209,"column":63}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":46},"generated":{"line":209,"column":65}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":11},"generated":{"line":209,"column":88}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":194,"column":4},"generated":{"line":209,"column":89}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":195,"column":3},"generated":{"line":210,"column":0}},{"source":"../../src/optimizers/optimizer_constructors.ts","name":null,"original":{"line":26,"column":34},"generated":{"line":212,"column":0}}],"sources":{"../../src/optimizers/optimizer_constructors.ts":"/**\n * @license\n * Copyright 2018 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {AdadeltaOptimizer} from './adadelta_optimizer';\nimport {AdagradOptimizer} from './adagrad_optimizer';\nimport {AdamOptimizer} from './adam_optimizer';\nimport {AdamaxOptimizer} from './adamax_optimizer';\nimport {MomentumOptimizer} from './momentum_optimizer';\nimport {RMSPropOptimizer} from './rmsprop_optimizer';\nimport {SGDOptimizer} from './sgd_optimizer';\n\nexport class OptimizerConstructors {\n  /**\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n   *\n   * ```js\n   * // Fit a quadratic function by learning the coefficients a, b, c.\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n   *\n   * const a = tf.scalar(Math.random()).variable();\n   * const b = tf.scalar(Math.random()).variable();\n   * const c = tf.scalar(Math.random()).variable();\n   *\n   * // y = a * x^2 + b * x + c.\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n   * const loss = (pred, label) => pred.sub(label).square().mean();\n   *\n   * const learningRate = 0.01;\n   * const optimizer = tf.train.sgd(learningRate);\n   *\n   * // Train the model.\n   * for (let i = 0; i < 10; i++) {\n   *   optimizer.minimize(() => loss(f(xs), ys));\n   * }\n   *\n   * // Make predictions.\n   * console.log(\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n   * const preds = f(xs).dataSync();\n   * preds.forEach((pred, i) => {\n   *   console.log(`x: ${i}, pred: ${pred}`);\n   * });\n   * ```\n   *\n   * @param learningRate The learning rate to use for the SGD algorithm.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static sgd(learningRate: number): SGDOptimizer {\n    return new SGDOptimizer(learningRate);\n  }\n\n  /**\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n   * descent.\n   *\n   * See\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\n   *\n   * @param learningRate The learning rate to use for the Momentum gradient\n   * descent algorithm.\n   * @param momentum The momentum to use for the momentum gradient descent\n   * algorithm.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static momentum(learningRate: number, momentum: number, useNesterov = false):\n      MomentumOptimizer {\n    return new MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n\n  /**\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n   * descent. This implementation uses plain momentum and is not centered\n   * version of RMSProp.\n   *\n   * See\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n   *\n   * @param learningRate The learning rate to use for the RMSProp gradient\n   * descent algorithm.\n   * @param decay The discounting factor for the history/coming gradient.\n   * @param momentum The momentum to use for the RMSProp gradient descent\n   * algorithm.\n   * @param epsilon Small value to avoid zero denominator.\n   * @param centered If true, gradients are normalized by the estimated\n   * variance of the gradient.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static rmsprop(\n      learningRate: number, decay = .9, momentum = 0.0, epsilon: number = null,\n      centered = false): RMSPropOptimizer {\n    return new RMSPropOptimizer(\n        learningRate, decay, momentum, epsilon, centered);\n  }\n\n  /**\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adam gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adam(\n      learningRate = 0.001, beta1 = 0.9, beta2 = 0.999,\n      epsilon: number = null): AdamOptimizer {\n    return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n   *\n   * @param learningRate The learning rate to use for the Adadelta gradient\n   * descent algorithm.\n   * @param rho The learning rate decay over each update.\n   * @param epsilon A constant epsilon used to better condition the grad\n   * update.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adadelta(learningRate = .001, rho = .95, epsilon: number = null):\n      AdadeltaOptimizer {\n    return new AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n\n  /**\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n   *\n   * @param learningRate The learning rate to use for the Adamax gradient\n   * descent algorithm.\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\n   * @param epsilon A small constant for numerical stability.\n   * @param decay The learning rate decay over each update.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adamax(\n      learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon: number = null,\n      decay = 0.0): AdamaxOptimizer {\n    return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n\n  /**\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n   * See\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n   * or\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n   *\n   * @param learningRate The learning rate to use for the Adagrad gradient\n   * descent algorithm.\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\n   * positive.\n   */\n  /**\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n   */\n  static adagrad(learningRate: number, initialAccumulatorValue = 0.1):\n      AdagradOptimizer {\n    return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n}\n"},"lineCount":null}},"error":null,"hash":"b240dd048a724c7b28fda177b4c4c041","cacheData":{"env":{}}}